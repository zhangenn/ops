{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Still partially relying on pandas, need to remove it completely once everything works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "from sqlalchemy import create_engine, MetaData, Table, update\n",
    "from sqlalchemy import Column, Integer, String, DateTime, ForeignKey\n",
    "from sqlalchemy.orm import sessionmaker, relationship\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.ext.automap import automap_base\n",
    "from sqlalchemy.engine.url import URL\n",
    "\n",
    "import arxiv\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used full file path for testing purpose\n",
    "json_file = pd.read_json(\"/Users/EmilyWang/Downloads/ops-master/paper-collector/DeepLearningArticles.json\", orient='index')\n",
    "json_file = json_file.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting information\n",
    "json_file['published_date'] = json_file['published'].str.extract('(\\d\\d\\d\\d-\\d\\d-\\d\\d)', expand=True)\n",
    "json_file['published_time'] = json_file['published'].str.extract('(\\d\\d:\\d\\d:\\d\\d)', expand=True)\n",
    "json_file['updated_date'] = json_file['updated'].str.extract('(\\d\\d\\d\\d-\\d\\d-\\d\\d)', expand=True)\n",
    "json_file['updated_time'] = json_file['updated'].str.extract('(\\d\\d:\\d\\d:\\d\\d)', expand=True)\n",
    "json_file['unique_id'] = json_file['id'].str.extract('(\\d\\d\\d\\d\\.\\d\\d\\d\\d\\d)', expand=True)\n",
    "json_file['version_number'] = json_file['id'].str.extract('(\\d$)', expand=True)\n",
    "\n",
    "final_json = json_file[['unique_id', 'version_number', 'author', \n",
    "                        'title', 'summary', 'arxiv_comment', \n",
    "                        'published_date', 'published_time', \n",
    "                        'updated_date', 'updated_time', \n",
    "                        'tags', 'authors']]\n",
    "\n",
    "final_json = final_json.drop_duplicates(subset='unique_id', keep='first', inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_new_articles():\n",
    "    new_articles = arxiv.query(search_query, max_results=5000,\n",
    "                               sort_by=\"lastUpdatedDate\", sort_order=\"descending\")\n",
    "    new_articles_df = pd.DataFrame.from_dict(new_articles)\n",
    "    ordered_new_articles = new_articles_df.reindex(\n",
    "                            columns=['title', 'author', 'authors', 'id', 'arxiv_comment',\n",
    "                                     'arxiv_primary_category', 'published', 'summary',\n",
    "                                     'tags', 'updated'])\n",
    "    return ordered_new_articles\n",
    "\n",
    "def extract_column(df_file):\n",
    "    # Extracting information\n",
    "    df_file['published_date'] = df_file['published'].str.extract('(\\d\\d\\d\\d-\\d\\d-\\d\\d)', expand=True)\n",
    "    df_file['published_time'] = df_file['published'].str.extract('(\\d\\d:\\d\\d:\\d\\d)', expand=True)\n",
    "    df_file['updated_date'] = df_file['updated'].str.extract('(\\d\\d\\d\\d-\\d\\d-\\d\\d)', expand=True)\n",
    "    df_file['updated_time'] = df_file['updated'].str.extract('(\\d\\d:\\d\\d:\\d\\d)', expand=True)\n",
    "    df_file['unique_id'] = df_file['id'].str.extract('(\\d\\d\\d\\d\\.\\d\\d\\d\\d\\d)', expand=True)\n",
    "    df_file['version_number'] = df_file['id'].str.extract('(\\d$)', expand=True)\n",
    "\n",
    "    final_df = df_file[['unique_id', 'version_number', 'author', \n",
    "                        'title', 'summary', 'arxiv_comment', \n",
    "                        'published_date', 'published_time', \n",
    "                        'updated_date', 'updated_time', \n",
    "                        'tags', 'authors']]\n",
    "\n",
    "    final_df = final_df.drop_duplicates(subset='unique_id', keep='first', inplace=False)\n",
    "    return final_df\n",
    "\n",
    "def check_existence(session, PaperTable, id_string):\n",
    "    query = session.query(PaperTable).filter(PaperTable.id==id_string)\n",
    "    \n",
    "    if query.one_or_none():\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def update_existing_articles(session, PaperTable, id_string, df):\n",
    "    now = datetime.now\n",
    "    update(PaperTable).where(PaperTable.id==id_string).\\\n",
    "            values(version = int(df.iloc[0, 1]), \\\n",
    "                   summary = df.iloc[0, 4], \\\n",
    "                   arxiv_comment = df.iloc[0, 5], \\\n",
    "                   updated_date = df.iloc[0, 8], \\\n",
    "                   updated_time = df.iloc[0, 9])\n",
    "\n",
    "    session.commit()\n",
    "\n",
    "def extract_category(df):\n",
    "    tags = df.iloc[0, 10]\n",
    "    tags_list = []\n",
    "    for i in range(len(tags)):\n",
    "        tags_list.append([tags[i]['term']])\n",
    "    return tags_list\n",
    "\n",
    "def insert_new_articles(session, PaperTable, AuthorTable, TagTable, id_string, df):\n",
    "    # Adding records into Paper Table\n",
    "    paper_row = PaperTable(id = id_string, \n",
    "                           version = int(df.iloc[0, 1]), \n",
    "                           author = df.iloc[0, 2], \n",
    "                           title = df.iloc[0, 3], \n",
    "                           summary = df.iloc[0, 4], \n",
    "                           arxiv_comment = df.iloc[0, 5], \n",
    "                           published_date = df.iloc[0, 6], \n",
    "                           published_time = df.iloc[0, 7], \n",
    "                           updated_date = df.iloc[0, 8], \n",
    "                           updated_time = df.iloc[0, 9])\n",
    "    session.add(paper_row)\n",
    "    session.commit()\n",
    "    \n",
    "    # Adding records into Author Table\n",
    "    authors = df.iloc[0, 11]\n",
    "    for i in range(len(authors)):\n",
    "        id_str = id_string + \"-\" + str(i)\n",
    "        author_row = AuthorTable(id = id_str, \n",
    "                                 author = authors[i], \n",
    "                                 author_entry = paper_row)\n",
    "        session.add(author_row)\n",
    "    session.commit()\n",
    "    \n",
    "    # Adding records into Tag Table\n",
    "    tags_list = extract_category(df)\n",
    "    for i in range(len(tags_list)):\n",
    "        id_str = id_string + \"-\" + str(i)\n",
    "        tag_row = TagTable(id = id_str, \n",
    "                           paper_tag = tags_list[i], \n",
    "                           tag_entry = paper_row)\n",
    "        session.add(tag_row)\n",
    "    session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local db url\n",
    "db_url = {'drivername': 'postgres',\n",
    "          'username': 'postgres',\n",
    "          'password': 'postgres',\n",
    "          'host': '127.0.0.1',\n",
    "          'port': 5432}\n",
    "engine = create_engine(URL(**db_url))\n",
    "\n",
    "# Initiate base\n",
    "Base = declarative_base()\n",
    "\n",
    "# Create tables\n",
    "class PaperTable(Base):\n",
    "    __tablename__ = 'PaperTable'\n",
    "    id = Column(String, primary_key=True)\n",
    "    version = Column(Integer, nullable=False)\n",
    "    author = Column(String, nullable=False)\n",
    "    authors = relationship('AuthorTable', \n",
    "                           backref='author_entry')\n",
    "    tags = relationship('TagTable', \n",
    "                        backref='tag_entry')\n",
    "    title = Column(String, nullable=False)\n",
    "    summary = Column(String)\n",
    "    arxiv_comment = Column(String)\n",
    "    published_date = Column(String)\n",
    "    published_time = Column(String)\n",
    "    updated_date = Column(String)\n",
    "    updated_time = Column(String)\n",
    "    timestamp = Column(DateTime, default=datetime.utcnow)\n",
    "\n",
    "class AuthorTable(Base):\n",
    "    __tablename__ = 'AuthorTable'\n",
    "    id = Column(String, primary_key=True)\n",
    "    author = Column(String, nullable=False)\n",
    "    paper_id = Column(String, ForeignKey('PaperTable.id'))\n",
    "\n",
    "class TagTable(Base):\n",
    "    __tablename__ = 'TagTable'\n",
    "    id = Column(String, primary_key=True)\n",
    "    paper_tag = Column(String, nullable=False)\n",
    "    paper_id = Column(String, ForeignKey('PaperTable.id'))\n",
    "\n",
    "Base.metadata.create_all(bind=engine)\n",
    "\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_json_lst = final_json['unique_id'].values\n",
    "\n",
    "for id_string in final_json_lst:\n",
    "    row_df = final_json[final_json['unique_id']==id_string]\n",
    "\n",
    "    if check_existence(session, PaperTable, id_string):\n",
    "        exist_version = session.query(PaperTable.version).filter(PaperTable.id==id_string)\n",
    "        if int(final_json[final_json['unique_id']==id_string]['version_number']) \\\n",
    "                > exist_version[0]:\n",
    "            update_existing_articles(session, PaperTable, id_string, row_df)\n",
    "\n",
    "    else:\n",
    "        insert_new_articles(session, PaperTable, AuthorTable, TagTable, id_string, row_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1803.08607 Mickey Aleksic {cs.CV}\n",
      "1803.08607 Liang Shen {cs.CV}\n",
      "1803.08607 Xiaopeng Zhang {cs.CV}\n",
      "1803.08607 Shaojie Zhuo {cs.CV}\n",
      "1803.08607 Chen Feng {cs.CV}\n",
      "1803.08607 Tao Sheng {cs.CV}\n",
      "1903.05071 Pavlos Protopapas {cs.NE}\n",
      "1903.05071 Nikos Gianniotis {cs.NE}\n",
      "1903.05071 Jacob Reinier Maat {cs.NE}\n",
      "1903.05071 Pavlos Protopapas {cs.LG}\n",
      "1903.05071 Nikos Gianniotis {cs.LG}\n",
      "1903.05071 Jacob Reinier Maat {cs.LG}\n",
      "1903.05071 Pavlos Protopapas {stat.ML}\n",
      "1903.05071 Nikos Gianniotis {stat.ML}\n",
      "1903.05071 Jacob Reinier Maat {stat.ML}\n",
      "1903.05063 Daniel Wintz {cs.LG}\n",
      "1903.05063 Elliott Wolf {cs.LG}\n",
      "1903.05063 Michael Lingzhi Li {cs.LG}\n",
      "1903.05063 Daniel Wintz {stat.ML}\n",
      "1903.05063 Elliott Wolf {stat.ML}\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import and_\n",
    "# Testing if foreign keys/relationships work\n",
    "query = session.query(PaperTable, AuthorTable, TagTable).filter(and_(PaperTable.id==AuthorTable.paper_id, \n",
    "                                                                PaperTable.id==TagTable.paper_id)).limit(20)\n",
    "for _p, _a, _t in query.all():\n",
    "    print(_p.id, _a.author, _t.paper_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing tables: ['PaperTable', 'AuthorTable', 'TagTable']\n",
      "Inserted 979 new articles, updated 0 existing articles.\n"
     ]
    }
   ],
   "source": [
    "### Complete\n",
    "# Import modules\n",
    "from sqlalchemy import create_engine, MetaData, Table, update\n",
    "from sqlalchemy import Column, Integer, String, DateTime, ForeignKey\n",
    "from sqlalchemy.orm import sessionmaker, relationship\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.ext.automap import automap_base\n",
    "from sqlalchemy.engine.url import URL\n",
    "\n",
    "import arxiv\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Keywords include deep learning, neural network, GPU, graphics processing unit,\n",
    "# reinforcement learning, OR perceptron\n",
    "keyword = '%28%22deep learning%22 OR %22neural network%22 \\\n",
    "            OR %22GPU%22 OR %22graphics processing unit%22 \\\n",
    "            OR %22reinforcement learning%22 OR %22perceptron%22%29'\n",
    "\n",
    "# Searches within machine learning (stats or cs), artificial intelligence or computer vision\n",
    "category = '%28cat:cs.LG OR cat:stat.ML OR cat:cs.AI OR cat:cs.CV%29'\n",
    "\n",
    "search_query = keyword + \" AND \" + category\n",
    "\n",
    "# Use create_tables when first setting up the database\n",
    "def create_tables():\n",
    "    # Initiate base\n",
    "    Base = declarative_base()\n",
    "    \n",
    "    # Create tables\n",
    "    class PaperTable(Base):\n",
    "        __tablename__ = 'PaperTable'\n",
    "        id = Column(String, primary_key=True)\n",
    "        version = Column(Integer, nullable=False)\n",
    "        author = Column(String, nullable=False)\n",
    "        authors = relationship('AuthorTable', \n",
    "                               backref='author_entry')\n",
    "        tags = relationship('TagTable', \n",
    "                            backref='tag_entry')\n",
    "        title = Column(String, nullable=False)\n",
    "        summary = Column(String)\n",
    "        arxiv_comment = Column(String)\n",
    "        published_date = Column(String)\n",
    "        published_time = Column(String)\n",
    "        updated_date = Column(String)\n",
    "        updated_time = Column(String)\n",
    "        timestamp = Column(DateTime, default=datetime.utcnow)\n",
    "        \n",
    "    class AuthorTable(Base):\n",
    "        __tablename__ = 'AuthorTable'\n",
    "        id = Column(String, primary_key=True)\n",
    "        author = Column(String, nullable=False)\n",
    "        paper_id = Column(String, ForeignKey('PaperTable.id'))\n",
    "        \n",
    "    class TagTable(Base):\n",
    "        __tablename__ = 'TagTable'\n",
    "        id = Column(String, primary_key=True)\n",
    "        paper_tag = Column(String, nullable=False)\n",
    "        paper_id = Column(String, ForeignKey('PaperTable.id'))\n",
    "        \n",
    "    Base.metadata.create_all(bind=engine)\n",
    "\n",
    "# max_results set to 1000\n",
    "def obtain_new_articles():\n",
    "    new_articles = arxiv.query(search_query, max_results=1000,\n",
    "                               sort_by=\"lastUpdatedDate\", \n",
    "                               sort_order=\"descending\")\n",
    "    new_articles_df = pd.DataFrame.from_dict(new_articles)\n",
    "    ordered_new_articles = new_articles_df.reindex(\n",
    "                            columns=['title', 'author', 'authors', 'id', 'arxiv_comment',\n",
    "                                     'arxiv_primary_category', 'published', 'summary',\n",
    "                                     'tags', 'updated'])\n",
    "    return ordered_new_articles\n",
    "\n",
    "def extract_column(df_file):\n",
    "    # Extracting information\n",
    "    df_file['published_date'] = df_file['published'].str.extract('(\\d\\d\\d\\d-\\d\\d-\\d\\d)', expand=True)\n",
    "    df_file['published_time'] = df_file['published'].str.extract('(\\d\\d:\\d\\d:\\d\\d)', expand=True)\n",
    "    df_file['updated_date'] = df_file['updated'].str.extract('(\\d\\d\\d\\d-\\d\\d-\\d\\d)', expand=True)\n",
    "    df_file['updated_time'] = df_file['updated'].str.extract('(\\d\\d:\\d\\d:\\d\\d)', expand=True)\n",
    "    df_file['unique_id'] = df_file['id'].str.extract('(\\d\\d\\d\\d\\.\\d\\d\\d\\d\\d)', expand=True)\n",
    "    df_file['version_number'] = df_file['id'].str.extract('(\\d$)', expand=True)\n",
    "\n",
    "    final_df = df_file[['unique_id', 'version_number', 'author', \n",
    "                        'title', 'summary', 'arxiv_comment', \n",
    "                        'published_date', 'published_time', \n",
    "                        'updated_date', 'updated_time', \n",
    "                        'tags', 'authors']]\n",
    "\n",
    "    final_df = final_df.drop_duplicates(subset='unique_id', \n",
    "                                        keep='first', inplace=False)\n",
    "    final_df = final_df.dropna(subset=['unique_id'], inplace=False)\n",
    "    return final_df\n",
    "\n",
    "def check_existence(session, PaperTable, id_string):\n",
    "    query = session.query(PaperTable).filter(PaperTable.c.id==id_string).all()\n",
    "    \n",
    "    if len(query) > 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def update_existing_articles(session, PaperTable, id_string, df):\n",
    "    update(PaperTable).where(PaperTable.c.id==id_string).\\\n",
    "            values(version = int(df.iloc[0, 1]), \\\n",
    "                   summary = df.iloc[0, 4], \\\n",
    "                   arxiv_comment = df.iloc[0, 5], \\\n",
    "                   updated_date = df.iloc[0, 8], \\\n",
    "                   updated_time = df.iloc[0, 9])\n",
    "\n",
    "    session.commit()\n",
    "\n",
    "def extract_category(df):\n",
    "    tags = df.iloc[0, 10]\n",
    "    tags_list = []\n",
    "    for i in range(len(tags)):\n",
    "        tags_list.append([tags[i]['term']])\n",
    "    return tags_list\n",
    "\n",
    "def insert_new_articles(session, PaperTable, AuthorTable, TagTable, \n",
    "                        id_string, df):\n",
    "    # Adding records into Paper Table\n",
    "    paper_row = PaperTable.insert().values(id = id_string, \n",
    "                                           version = int(df.iloc[0, 1]), \n",
    "                                           author = df.iloc[0, 2], \n",
    "                                           title = df.iloc[0, 3], \n",
    "                                           summary = df.iloc[0, 4], \n",
    "                                           arxiv_comment = df.iloc[0, 5], \n",
    "                                           published_date = df.iloc[0, 6], \n",
    "                                           published_time = df.iloc[0, 7], \n",
    "                                           updated_date = df.iloc[0, 8], \n",
    "                                           updated_time = df.iloc[0, 9])\n",
    "    session.commit()\n",
    "    \n",
    "    # Adding records into Author Table\n",
    "    authors = df.iloc[0, 11]\n",
    "    for i in range(len(authors)):\n",
    "        id_str = id_string + \"-\" + str(i)\n",
    "        author_row = AuthorTable.insert().values(id = id_str, \n",
    "                                                 author = authors[i], \n",
    "                                                 author_entry = paper_row)\n",
    "    session.commit()\n",
    "    \n",
    "    # Adding records into Tag Table\n",
    "    tags_list = extract_category(df)\n",
    "    for i in range(len(tags_list)):\n",
    "        id_str = id_string + \"-\" + str(i)\n",
    "        tag_row = TagTable.insert().values(id = id_str, \n",
    "                                           paper_tag = tags_list[i], \n",
    "                                           tag_entry = paper_row)\n",
    "    session.commit()\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Local db url\n",
    "    db_url = {'drivername': 'postgres',\n",
    "              'username': 'postgres',\n",
    "              'password': 'postgres',\n",
    "              'host': '127.0.0.1',\n",
    "              'port': 5432}\n",
    "    engine = create_engine(URL(**db_url))\n",
    "    connection = engine.connect()\n",
    "    print(\"Existing tables:\", engine.table_names())\n",
    "\n",
    "    # reflect the tables\n",
    "    metadata = MetaData()\n",
    "    Base = automap_base(metadata=metadata)\n",
    "    Base.prepare(engine, reflect=True)\n",
    "\n",
    "    # Mapped classes with names matching that of the table name\n",
    "    PaperTable = Table('PaperTable', metadata, autoload=True, \n",
    "                       autoload_with=engine)\n",
    "    AuthorTable = Table('AuthorTable', metadata, autoload=True, \n",
    "                        autoload_with=engine)\n",
    "    TagTable = Table('TagTable', metadata, autoload=True, \n",
    "                     autoload_with=engine)\n",
    "    \n",
    "    Session = sessionmaker(bind=engine)\n",
    "    session = Session()\n",
    "    \n",
    "    article_df = extract_column(obtain_new_articles())\n",
    "    #article_df = extract_column(pd.read_json(\"/Users/EmilyWang/Desktop/ops/paper-collector/DeepLearningArticles.json\", \n",
    "    #                                         orient='index'))\n",
    "    article_id_lst = article_df['unique_id'].values\n",
    "    \n",
    "    count_update = 0\n",
    "    count_insert = 0\n",
    "\n",
    "    for id_string in article_id_lst:\n",
    "        row_df = article_df[article_df['unique_id']==id_string]\n",
    "        \n",
    "        if check_existence(session, PaperTable, id_string):\n",
    "            exist_version = session.query(PaperTable.c.version).\\\n",
    "                                    filter(PaperTable.c.id==id_string).one()\n",
    "            if int(article_df.iloc[0, 1]) > exist_version[0]:\n",
    "                update_existing_articles(session, \n",
    "                                         PaperTable, id_string, row_df)\n",
    "                count_update = count_update + 1\n",
    "                \n",
    "        else:\n",
    "            insert_new_articles(session, PaperTable, AuthorTable, \n",
    "                                TagTable, id_string, row_df)\n",
    "            count_insert = count_insert + 1\n",
    "            \n",
    "    print(\"Inserted {} new articles, updated {} existing articles.\".format(count_insert, count_update))\n",
    "    session.close()\n",
    "\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import func\n",
    "rows = session.query(func.count(PaperTable.id)).scalar()\n",
    "auth = session.query(func.count(AuthorTable.id)).scalar()\n",
    "tags = session.query(func.count(TagTable.id)).scalar()\n",
    "print(rows, auth, tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import and_\n",
    "# Testing if foreign keys/relationships work\n",
    "query = session.query(PaperTable, AuthorTable, TagTable).filter(and_(PaperTable.id==AuthorTable.paper_id, \n",
    "                                                                PaperTable.id==TagTable.paper_id)).limit(10)\n",
    "for _p, _a, _t in query.all():\n",
    "    print(_p.id, _a.author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
